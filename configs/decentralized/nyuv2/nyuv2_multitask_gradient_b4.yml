# B4: Multi-task Pure Gradient v2
# Task Discovery Experiment 4: Pure gradient similarity in multi-task setting
#
# This configuration combines:
# Multi-task client distribution (from A2)
# Pure gradient similarity alpha=0.0 (from B1)
# Full aggregation (same as A2, for direct comparison)
# Early stopping and LR scheduler
#
# Scientific Questions:
# 1. B4 vs A2: Can automatic discovery match manual design in multi-task scenarios?
# 2. B4 vs B1: Does multi-task help automatic discovery?
# 3. (B4 - B1) vs (A2 - A1): Do both methods capture same multi-task benefit?

general:
  title: "b4_multitask_pure_grad_v2"
  seed: 42
  description: "B4 v2: Multi-task clients with pure gradient similarity (alpha=0.0)"

data:
  dataset: "nyuv2"
  root_dir: "./data/nyuv2"
  dataset_fraction: 1.0

setup:
  num_clients: 6
  n_neighbors: 3
  alpha: 0.0  # KEY: Pure gradient similarity (no task prior knowledge)
  aggregate_heads: true  # Full aggregation (same as A2 for fair comparison)
  task_threshold: 0.1
  aggregation_method: "weighted"

  # Pairwise task combinations (same as A2)
  # Each client handles 2 tasks with 0.7/0.3 split
  task_weights_per_client:
    # C0: depth-dominant + segmentation
    - depth: 0.7
      segmentation: 0.3
      normal: 0.0

    # C1: depth-dominant + normal
    - depth: 0.7
      segmentation: 0.0
      normal: 0.3

    # C2: segmentation-dominant + depth
    - depth: 0.3
      segmentation: 0.7
      normal: 0.0

    # C3: segmentation-dominant + normal
    - depth: 0.0
      segmentation: 0.7
      normal: 0.3

    # C4: normal-dominant + depth
    - depth: 0.3
      segmentation: 0.0
      normal: 0.7

    # C5: normal-dominant + segmentation
    - depth: 0.0
      segmentation: 0.3
      normal: 0.7

training:
  num_rounds: 50
  local_epochs: 5
  batch_size: 4
  learning_rate: 0.001
  optimizer: "adam"

  # Learning rate scheduler
  lr_scheduler:
    enabled: true
    type: "cosine"
    min_lr: 0.0001
    warmup_rounds: 3

  # Early stopping - balanced settings
  early_stopping:
    enabled: true
    patience: 10  # Same as A2 (multi-task can be noisy)
    min_delta: 0.002  # Same as A2
    restore_best_weights: true
    verbose: true
    mode: "min"

model:
  backbone: "resnet50"
  pretrained: true
  num_seg_classes: 13
  out_size: [288, 384]

output:
  output_dir: "./results/nyuv2/b4_multitask_pure_grad_v2"
  save_checkpoints: true
  checkpoint_frequency: 5
  log_frequency: 1
  save_metrics_every_round: true

# Expected results (hypotheses):
#
# Scenario 1 (Optimistic): B4 ~= A2
#   - Automatic discovery captures task relationships as well as manual design
#   - Best loss: ~0.69-0.73
#   - This would be the strongest result! 
#
# Scenario 2 (Conservative): B4 ~= B1
#   - Multi-task benefit canceled out by lack of task prior (alpha=0.0)
#   - Best loss: ~0.72-0.76
#   - Still shows automatic discovery works, but multi-task doesn't help
#
# Scenario 3 (Middle): B4 between A2 and B1
#   - Partial multi-task benefit captured
#   - Best loss: ~0.70-0.74
#   - Shows promise but room for improvement
#
# Scenario 4 (Pessimistic): B4 >> B1
#   - Multi-task + no prior knowledge = gradient conflicts
#   - Best loss: >0.80
#   - Would need explanation and mitigation strategies
#
# Key comparisons to analyze:
# 1. B4 vs A2: gap = automatic discovery penalty in multi-task
# 2. B4 vs B1: delta = multi-task benefit under automatic discovery
# 3. (A2-A1) vs (B4-B1): consistency of multi-task benefit
